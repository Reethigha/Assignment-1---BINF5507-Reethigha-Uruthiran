# import all necessary libraries here
import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler, StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, accuracy_score


import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler, StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, accuracy_score

messy_data = pd.read_csv('../Data/messy_data.csv')
clean_data = messy_data.copy()


# 1. Impute Missing Values
# def impute_missing_values(input_data, strategy='mean'):
#     numeric_col = input_data.select_dtypes(include='number').columns  # Numeric columns
#     categorical_cols = input_data.select_dtypes(exclude='number').columns  # Categorical columns

#     if strategy == 'mean':
#         input_data[numeric_col] = input_data[numeric_col].fillna(input_data[numeric_col].mean())
#     elif strategy == 'median':
#         input_data[numeric_col] = input_data[numeric_col].fillna(input_data[numeric_col].median())
#     elif strategy == 'mode':
#         for col in categorical_cols:
#             input_data[col] = input_data[col].fillna(input_data[col].mode()[0])
#     elif strategy == 'missing value':
#         input_data = input_data.fillna('missing value')  # Fill all missing values with 'missing value'
#     else:
#         raise ValueError("Please enter a valid strategy: 'mean', 'median', 'mode', or 'missing value'.")
    
#     return input_data
    
def impute_missing_values(input_data, strategy='mean'):
    numeric_col = input_data.select_dtypes(include='number').columns  # interprets numeric columns
    categorical_cols = input_data.select_dtypes(exclude='number').columns  # interperets categorical columns

    if strategy == 'mean':
        input_data[numeric_col] = input_data[numeric_col].fillna(input_data[numeric_col].mean())
    elif strategy == 'median':
        input_data[numeric_col] = input_data[numeric_col].fillna(input_data[numeric_col].median())
    elif strategy == 'mode':
        input_data[numeric_col] = input_data[numeric_col].fillna(input_data[numeric_col].mode()[0])
        for col in categorical_cols:
            input_data[col] = input_data[col].fillna(input_data[col].mode()[0])
    elif strategy == 'missing value':
        input_data = input_data.fillna('missing value')  # Fill all missing values with 'missing value'
    else:
        raise ValueError("Please enter a valid strategy: 'mean', 'median', 'mode', or 'missing value'.")
    
    return input_data

# %%
mean_missing = impute_missing_values(messy_data, strategy = 'mean')
print(mean_missing.head())

median_missing = impute_missing_values(messy_data, strategy = 'median')
print(median_missing.head())

mode_missing = impute_missing_values(messy_data, strategy = 'mode')
print(mode_missing.head())


# 2. Remove Duplicates
def remove_duplicates(data):
    return data.drop_duplicates()

# %%
rem_duplicate = remove_duplicates(messy_data)
print(rem_duplicate.head())

# 3. Normalize Numerical Data
def normalize_data(data, method='minmax'):
    num_data = data.select_dtypes(include=['number'])
    
    if method == 'minmax':
        scaler = MinMaxScaler()
    elif method == 'standard':
        scaler = StandardScaler()
    else:
        raise ValueError("Please choose 'minmax' or 'standard'.")
    
    normalized_data = scaler.fit_transform(num_data) #numerical data will be transformed using the chosen scaler.
    return pd.DataFrame(normalized_data, columns = num_data.columns)

#%%
norm_data = normalize_data(messy_data, method='minmax')
print(norm_data.head())

# 4. Remove Redundant Features   
def remove_redundant_features(data, threshold=0.9):
    numeric_data = data.select_dtypes(include=['number']) #numeric columns are selected
    data_corr = numeric_data.corr().abs() #calculates the absolute correlation for all numerical columns.
    drop_redun = set() #stores the redundant columns names that need to be removed and prevents duplicate entries
    for i in range(len(data_corr.columns)): #index of first column is represented by i
       for s in range(i + 1, len(data_corr.columns)): # second column in a pair being compared is represented by s and 'i+1' avoids comparing the column with itself
          if data_corr.iloc[i, s] > threshold: #checks whether the correlation between columns i and s are above the threshold 
             col_name = data_corr.columns[s] #retrives the name of the column which associates with index j.
             drop_redun.add(col_name) # adds the name of the redundant column (with index j) to 'drop_redun'
    return data.drop(columns=drop_redun) #removes the redundant columns from the dataset.

#%%
rem_redun_feat = remove_redundant_features(messy_data, threshold=0.9)
print(rem_redun_feat.head())

# # ---------------------------------------------------

def simple_model(input_data, split_data=True, scale_data=False, print_report=False):
#     """
#     A simple logistic regression model for target classification.
#     Parameters:
#     input_data (pd.DataFrame): The input data containing features and the target variable 'target' (assume 'target' is the first column).
#     split_data (bool): Whether to split the data into training and testing sets. Default is True.
#     scale_data (bool): Whether to scale the features using StandardScaler. Default is False.
#     print_report (bool): Whether to print the classification report. Default is False.
#     Returns:
#     None
#     The function performs the following steps:
#     1. Removes columns with missing data.
#     2. Splits the input data into features and target.
#     3. Encodes categorical features using one-hot encoding.
#     4. Splits the data into training and testing sets (if split_data is True).
#     5. Scales the features using StandardScaler (if scale_data is True).
#     6. Instantiates and fits a logistic regression model.
#     7. Makes predictions on the test set.
#     8. Evaluates the model using accuracy score and classification report.
#     9. Prints the accuracy and classification report (if print_report is True).
#     """

    # if there's any missing data, remove the columns
    input_data.dropna(inplace=True)

    # split the data into features and target
    target = input_data.copy()[input_data.columns[0]]
    features = input_data.copy()[input_data.columns[1:]]

    # if the column is not numeric, encode it (one-hot)
    for col in features.columns:
        if features[col].dtype == 'object':
            features = pd.concat([features, pd.get_dummies(features[col], prefix=col)], axis=1)
            features.drop(col, axis=1, inplace=True)

    X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, stratify=target, random_state=42)

    if scale_data:
        # scale the data
        X_train = normalize_data(X_train)
        X_test = normalize_data(X_test)
        
    # instantiate and fit the model
    log_reg = LogisticRegression(random_state=42, max_iter=100, solver='liblinear', penalty='l2', C=1.0)
    log_reg.fit(X_train, y_train)

    # make predictions and evaluate the model
    y_pred = log_reg.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    report = classification_report(y_test, y_pred)

    print(f'Accuracy: {accuracy}')
    
    # if specified, print the classification report
    if print_report:
        print('Classification Report:')
        print(report)
        print('Read more about the classification report: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html and https://www.nb-data.com/p/breaking-down-the-classification')
    
    return None


